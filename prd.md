## éœ€æ±‚

### ä¸€ã€æ¨¡å‹éƒ¨ç½²ä¸é…ç½®

1.  ä½¿ç”¨ vLLM æœ¬åœ°éƒ¨ç½² `Qwen3-32B-AWQ` æ¨¡å‹ï¼š[https://www.modelscope.cn/models/Qwen/Qwen3-32B-AWQ/summary](https://www.modelscope.cn/models/Qwen/Qwen3-32B-AWQ/summary)
2.  å®ç° 32,768 ä¸ª tokens çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œ OpenAI å…¼å®¹ APIï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚
3.  é€šè¿‡ç²¾ç»†è°ƒæ•´éƒ¨ç½²å‚æ•°ï¼Œåœ¨æŒ‡å®šçš„ GPU èµ„æºä¸‹æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ã€‚
4.  æ¨¡å‹æ–‡ä»¶å·²ç»ä¸‹è½½åˆ°æœ¬åœ°ï¼š`/home/llm/model/qwen/Qwen3-32B-AWQ`
5.  è®¾ç½®ï¼š`enable_thinking=False`
    *   è¿™ç§æ¨¡å¼åœ¨éœ€è¦é€šè¿‡ç¦ç”¨æ€è€ƒæ¥æé«˜æ•ˆç‡çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚
    *   ç¤ºä¾‹ä»£ç ï¼š
        ```python
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # Setting enable_thinking=False disables thinking mode
        )
        ```
    *   åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¸ä¼šç”Ÿæˆä»»ä½•æ€è€ƒå†…å®¹ï¼Œä¹Ÿä¸ä¼šåŒ…å« `<think>...</think>` å—ã€‚
6.  å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç« ï¼š[https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/](https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/)

### äºŒã€ç³»ç»Ÿè¦æ±‚ ğŸ–¥ï¸

#### ç¡¬ä»¶é…ç½®

*   4 å— NVIDIA GPU (æ¯å— 22GB æ˜¾å­˜ï¼Œæ€»è®¡ 88GB)
*   512GB ç³»ç»Ÿå†…å­˜
*   2TB SSD å­˜å‚¨
*   56 æ ¸ CPU

#### è½¯ä»¶ç¯å¢ƒ

*   Ubuntu 24.04
*   NVIDIA-SMI `570.153.02` (Driver Version: `570.153.02`, CUDA Version: `12.8`)
*   Docker Image: `vllm/vllm-openai:v0.8.5`
